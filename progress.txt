# NanoChat .NET - Progress Log

## Phase 1: Project Foundation - COMPLETE ✅

### Date: February 5, 2026

### Completed Stories:

**Story 1.1: Create solution and project structure**
- Created NanoChat.sln solution file
- Created NanoChat.Core class library (src/NanoChat.Core)
- Created NanoChat.CLI console application (src/NanoChat.CLI)
- All projects target .NET 9.0 (LTS)
- Solution builds successfully with no warnings

**Story 1.2: Add TorchSharp-cpu dependency**
- Added TorchSharp-cpu v0.105.2 to Core and CLI projects
- Created TensorExample class with tensor creation and matmul tests
- CLI application successfully creates tensors and runs matrix multiplication
- Verified TorchSharp integration works correctly

**Story 1.3: Add test project**
- Created NanoChat.Core.Tests xUnit test project (tests/NanoChat.Core.Tests)
- Added tests for TensorExample (CanCreateTensor, CanRunMatmul)
- All tests pass successfully
- Test project added TorchSharp-cpu reference for native library access

### Technical Notes:

**Framework:**
- Using .NET 9.0 (LTS) instead of .NET 10 for better library compatibility
- TorchSharp-cpu v0.105.2 with libtorch 2.7.1

**Dependencies:**
- TorchSharp-cpu (includes native libtorch libraries for CPU)
- xUnit for testing

**Known Requirements:**
- macOS users need libomp installed: `brew install libomp`
- Native libraries are automatically copied to output directory
- All builds complete with 0 warnings
- All tests pass (2 passing)

### Build & Test Commands:

```bash
# Build
dotnet build NanoChat.sln

# Run CLI
dotnet run --project src/NanoChat.CLI/NanoChat.CLI.csproj

# Run Tests
dotnet test NanoChat.sln
```

### Next Steps:

Ready to proceed with Phase 1, Feature 2: Tokenizer
- Story 2.1: Create tokenizer interface
- Story 2.2: Implement tiktoken-based tokenizer
- Story 2.3: Add special token support
- Story 2.4: Load nanochat tokenizer from disk
- Story 2.5: Test against Python implementation

### Git Commit:

Commit: 35dd12e
Message: "feat: complete Phase 1 - project foundation with TorchSharp integration"

---

## Phase 1: Feature 2 - Tokenizer - COMPLETE ✅

### Date: February 5, 2026

### Completed Stories:

**Story 2.1: Create tokenizer interface**
- Created ITokenizer interface in NanoChat.Core/Tokenizer/ITokenizer.cs
- Interface includes Encode() and Decode() methods
- Added properties for VocabSize, BosToken, and EosToken
- Support for special tokens via allowedSpecial parameter

**Story 2.2: Implement tiktoken-based tokenizer**
- Implemented BpeTokenizer class with byte-level BPE encoding
- Uses mergeable ranks dictionary for token merging
- Implements proper BPE algorithm with pair merging
- Handles UTF-8 encoding/decoding correctly
- Custom ByteArrayComparer for efficient byte array dictionary operations

**Story 2.3: Add special token support**
- Created TokenizerFactory with nanochat-specific special tokens:
  - <|bos|> (beginning of sequence)
  - <|eos|> (end of sequence)
  - <|user_start|>, <|user_end|>
  - <|assistant_start|>, <|assistant_end|>
  - <|system_start|>, <|system_end|>
- Special tokens properly integrated into encoding/decoding flow
- Support for custom special token sets

**Story 2.4: Load nanochat tokenizer from disk**
- Created TokenizerLoader with multiple format support:
  - LoadFromTiktoken() for tiktoken-style text format
  - LoadFromJson() for JSON format with base64-encoded ranks
- Placeholder for future pickle format support
- Handles invalid/malformed data gracefully

**Story 2.5: Test against Python implementation**
- Created comprehensive test suite with 10 tests:
  - Basic encoding/decoding
  - Round-trip encoding tests
  - Special token handling
  - Conversation format testing
  - Unicode/emoji support
  - Empty string handling
- All tests pass successfully

### Technical Implementation:

**Architecture:**
```
ITokenizer (interface)
    └── BpeTokenizer (implementation)
        ├── Byte-level BPE encoding
        ├── Special token support
        └── UTF-8 text handling

TokenizerFactory
    └── Creates tokenizers with predefined special tokens

TokenizerLoader
    └── Loads tokenizer data from disk (tiktoken/JSON formats)
```

**Key Features:**
- Full BPE implementation with proper merge ordering
- Special token injection during encoding
- Robust Unicode handling
- Efficient byte array operations
- Flexible loading from multiple formats

### Test Results:
- All 10 tokenizer tests pass
- No build warnings
- Proper round-trip encoding/decoding
- Special token handling verified

### Files Created:
- src/NanoChat.Core/Tokenizer/ITokenizer.cs
- src/NanoChat.Core/Tokenizer/BpeTokenizer.cs
- src/NanoChat.Core/Tokenizer/TokenizerFactory.cs
- src/NanoChat.Core/Tokenizer/TokenizerLoader.cs
- tests/NanoChat.Core.Tests/TokenizerTests.cs

### Files Removed:
- src/NanoChat.Core/Class1.cs (replaced by tokenizer implementation)
- tests/NanoChat.Core.Tests/UnitTest1.cs (replaced by TokenizerTests.cs)

### Updated:
- src/NanoChat.CLI/Program.cs (updated to demonstrate tokenizer functionality)
- docs/plan.md (marked Feature 2 stories as complete)

### Next Steps:

Ready to proceed with Feature 3: Model Components
- Story 3.1: Implement RMSNorm
- Story 3.2: Implement Rotary Embeddings
- Story 3.3: Implement Multi-Head Attention
- Story 3.4: Implement GQA (Group-Query Attention)
- Story 3.5: Implement MLP block
- Story 3.6: Implement Transformer Block
- Story 3.7: Implement Value Embeddings

### Git Commit:

Commit: 1cdeb0c
Message: "feat: implement BPE tokenizer with special token support"

---

## Phase 1: Feature 3 - Model Components (Story 3.1)

### Date: February 5, 2026

### Completed Stories:

**Story 3.1: Implement RMSNorm**
- Created RMSNorm class in NanoChat.Core/Model/RMSNorm.cs
- Implements Root Mean Square Layer Normalization without learnable parameters
- Formula: RMSNorm(x) = x / sqrt(mean(x²) + eps)
- Normalizes over the last dimension of input tensor
- Configurable epsilon for numerical stability (default: 1e-6)

### Technical Implementation:

**Class Structure:**
- Inherits from TorchSharp's Module<Tensor, Tensor>
- No learnable parameters (matches nanochat's F.rms_norm())
- Efficient tensor operations using TorchSharp
- Proper disposal of resources

**Key Features:**
- Normalizes input tensors to RMS ≈ 1.0
- Handles batched inputs correctly
- Works with arbitrary tensor dimensions
- Numerical stability with epsilon parameter

### Test Results:
- Created comprehensive test suite with 6 tests:
  - Simple input normalization verification
  - Batched input (independent normalization per sample)
  - 3D input (last dimension normalization)
  - Zero input handling
  - RMS verification (output RMS ≈ 1.0)
  - Epsilon parameter validation
- All 16 tests pass (10 tokenizer + 6 RMSNorm)
- Build completes with 0 warnings

### Files Created:
- src/NanoChat.Core/Model/RMSNorm.cs
- tests/NanoChat.Core.Tests/RMSNormTests.cs

### Files Removed:
- verify_rmsnorm.py (temporary test file)

### Updated:
- docs/plan.md (marked Story 3.1 as complete)

### Next Steps:

Ready to proceed with Story 3.2: Implement Rotary Embeddings (RoPE)
- Precompute cos/sin tables
- Apply rotation to Q/K tensors
- Base frequency: 10000

### Git Commit:

Commit: a658d36
Message: "feat: implement Multi-Head Attention with GQA support (Story 3.3)"

---

## Phase 1: Feature 3 - Model Components (Story 3.5)

### Date: February 5, 2026

### Completed Stories:

**Story 3.5: Implement MLP block**
- Created MLP class in NanoChat.Core/Model/MLP.cs
- Implements two-layer feedforward network with ReLU² activation
- ReLU² activation: relu(x)² - apply ReLU then square the result
- Configurable hidden dimension (defaults to 4x embedding dimension)
- No bias terms in linear layers (matches nanochat spec)

### Technical Implementation:

**Class Structure:**
- Inherits from TorchSharp's Module<Tensor, Tensor>
- Two linear layers: nEmbd -> hiddenDim -> nEmbd
- ReLU² activation between layers
- No learnable bias parameters
- Proper resource disposal

**Key Features:**
- ReLU² activation as specified in nanochat (relu then square)
- Configurable hidden dimension with sensible default (4x embedding)
- Efficient tensor operations using TorchSharp functional API
- Supports arbitrary batch sizes and sequence lengths
- Memory-efficient with no unnecessary intermediate tensors

### Test Results:
- Created comprehensive test suite with 11 tests:
  - Default hidden dimension (4x embedding)
  - Custom hidden dimension
  - Simple input shape verification
  - ReLU² activation behavior
  - Zero input handling
  - Batched input processing
  - Different inputs produce different outputs
  - Repeated calls produce same output (determinism)
  - ReLU² correctness verification
  - Resource disposal
  - Large embedding dimensions (GPT-2 style: 768 -> 3072)
- All 49 tests pass (10 tokenizer + 6 RMSNorm + 9 RoPE + 13 Attention + 11 MLP)
- Build completes with 0 warnings

### Files Created:
- src/NanoChat.Core/Model/MLP.cs
- tests/NanoChat.Core.Tests/MLPTests.cs

### Updated:
- docs/plan.md (marked Story 3.5 as complete)

### Next Steps:

Ready to proceed with Story 3.6: Implement Transformer Block
- Combine Attention + MLP with residual connections
- Add layer-wise residual lambdas
- Integrate RMSNorm for pre-normalization

### Git Commit:

Commit: 6ffe8a0
Message: "feat: implement MLP block with ReLU² activation (Story 3.5)"

---


## Phase 1: Feature 3 - Model Components (Story 3.2)

### Date: February 5, 2026

### Completed Stories:

**Story 3.2: Implement Rotary Embeddings (RoPE)**
- Created RotaryEmbedding class in NanoChat.Core/Model/RotaryEmbedding.cs
- Implements Rotary Position Embedding for encoding position information
- Precomputes cos/sin rotation matrices for all positions
- Applies rotation to Q and K tensors in attention
- Base frequency: 10000 (configurable)

### Technical Implementation:

**Class Structure:**
- Inherits from TorchSharp's Module<Tensor, long, Tensor>
- Precomputes inverse frequencies: 1 / (base^(2i/dim)) for i in [0, dim/2)
- Caches cos and sin values for maximum sequence length
- Applies rotary transformation: [x1*cos - x2*sin, x2*cos + x1*sin]

**Key Features:**
- Efficient precomputation of rotation matrices
- Position-dependent rotations preserve relative position information
- Supports arbitrary sequence lengths (up to max)
- Rotation preserves vector norms (rotation is an orthogonal transform)
- Configurable base frequency for different frequency patterns

### Test Results:
- Created comprehensive test suite with 9 tests:
  - Constructor initialization verification
  - Shape preservation after rotation
  - Norm preservation (rotation maintains magnitude)
  - Different positions produce different rotations
  - Multiple sequence lengths handling
  - Rotation formula verification
  - Expected rotation behavior across positions
  - Dimension splitting correctness
  - Base parameter effect on frequency
- All 25 tests pass (10 tokenizer + 6 RMSNorm + 9 RoPE)
- Build completes with 0 warnings

### Files Created:
- src/NanoChat.Core/Model/RotaryEmbedding.cs
- tests/NanoChat.Core.Tests/RotaryEmbeddingTests.cs

### Updated:
- docs/plan.md (marked Story 3.2 as complete)

### Git Commit:

Commit: 8b12f94
Message: "feat: implement Rotary Position Embeddings (RoPE) (Story 3.2)"

---

## Phase 1: Feature 3 - Model Components (Story 3.3)

### Date: February 5, 2026

### Completed Stories:

**Story 3.3: Implement Multi-Head Attention**
- Created Attention class in NanoChat.Core/Model/Attention.cs
- Implements Multi-Head Causal Self-Attention with GQA support
- Supports Group-Query Attention (GQA) where n_kv_head < n_head
- Causal masking prevents attending to future tokens
- QK normalization applied after RoPE (per nanochat spec)
- Rotary position embeddings integrated
- Optional sliding window attention support

### Technical Implementation:

**Class Structure:**
- Inherits from TorchSharp's Module<Tensor, long, Tensor>
- Q projection: nHead heads
- K/V projections: nKvHead heads (supports GQA)
- QK normalization via RMSNorm (applied after RoPE)
- Rotary position embeddings for position encoding
- Scaled dot-product attention with scaling factor: 1/sqrt(headDim)

**Key Features:**
- Full Multi-Head Attention (MHA) support
- Group-Query Attention (GQA) with configurable K/V heads
- Multi-Query Attention (MQA) when nKvHead = 1
- Causal masking for autoregressive generation
- Optional sliding window attention for local attention
- Proper broadcasting for GQA (K/V head expansion)
- Efficient matmul operations

### Test Results:
- Created comprehensive test suite with 13 tests:
  - Valid parameter initialization
  - Invalid parameter validation (nEmbd not divisible by nHead)
  - Invalid parameter validation (nHead not divisible by nKvHead)
  - GQA support verification
  - Output shape verification (MHA)
  - Output shape verification (GQA)
  - Causal masking behavior
  - Sliding window attention
  - Multiple sequence lengths handling
  - Zero input handling
  - Single token handling
  - Large batch processing
  - Different head configurations (MHA, GQA, MQA)
- All 38 tests pass (10 tokenizer + 6 RMSNorm + 9 RoPE + 13 Attention)
- Build completes with 0 warnings

### Files Created:
- src/NanoChat.Core/Model/Attention.cs
- tests/NanoChat.Core.Tests/AttentionTests.cs

### Files Updated:
- src/NanoChat.Core/Model/RotaryEmbedding.cs (fixed Float32 dtype consistency)

### Bug Fixes:
- Fixed RotaryEmbedding to use Float32 (float) instead of Float64 (double)
  - Changed invFreq tensor creation to use float array
  - Prevents dtype mismatch errors in matmul operations
  - Ensures consistency across all tensor operations

### Updated:
- docs/plan.md (marked Story 3.3 as complete)

### Next Steps:

Ready to proceed with Story 3.4: Implement GQA (Group-Query Attention)
- Note: GQA is already implemented in the Attention class
- Story 3.4 can be marked as complete
- Next real work: Story 3.5: Implement MLP block

---

## Phase 1: Feature 3 - Model Components (Story 3.6)

### Date: February 5, 2026

### Completed Stories:

**Story 3.6: Implement Transformer Block**
- Created TransformerBlock class in NanoChat.Core/Model/TransformerBlock.cs
- Implements standard transformer architecture with pre-normalization
- Combines Attention and MLP sub-layers with residual connections
- Supports per-layer residual scaling via `residLambda` parameter
- Supports ResFormer-style residuals via `x0Lambda` parameter
- Optional sliding window attention and GQA support

### Technical Implementation:

**Class Structure:**
- Inherits from TorchSharp's Module<Tensor, long, Tensor?, Tensor>
- Pre-normalization: RMSNorm applied before each sub-layer
- Attention sub-layer: Multi-head causal self-attention with optional GQA
- MLP sub-layer: Feed-forward network with ReLU² activation
- Residual connections with configurable scaling factors

**Architecture:**
```
x1 = x + residLambda * attention(norm(x)) + x0Lambda * x0
x2 = x1 + residLambda * mlp(norm(x1)) + x0Lambda * x0
```

**Key Features:**
- Pre-normalization architecture (norm before sub-layer)
- Residual connection scaling via `residLambda` (default: 1.0)
- ResFormer-style residuals via `x0Lambda` (default: 0.0)
- Flexible configuration: GQA, sliding window, custom hidden dim
- Proper resource disposal and module registration
- Full integration with existing Attention and MLP components

### Test Results:
- Created comprehensive test suite with 15 tests:
  - Valid parameter initialization
  - Shape verification (simple input, GQA, sliding window)
  - Custom hidden dimension configuration
  - Residual lambda scaling behavior
  - ResFormer x0_lambda residual application
  - x0=null handling (ignores x0_lambda)
  - Zero input handling
  - Single token and large batch processing
  - Different inputs produce different outputs
  - Repeated calls produce same output (deterministic)
  - GPT-style configuration (768 dim, 12 heads)
  - Resource disposal
- All 64 tests pass (10 tokenizer + 6 RMSNorm + 9 RoPE + 13 Attention + 11 MLP + 15 TransformerBlock)
- Build completes with 0 warnings

### Files Created:
- src/NanoChat.Core/Model/TransformerBlock.cs
- tests/NanoChat.Core.Tests/TransformerBlockTests.cs

### Updated:
- docs/plan.md (marked Story 3.6 as complete)

### Next Steps:

Ready to proceed with Story 3.7: Implement Value Embeddings
- ResFormer-style alternating value embeddings
- Gated value embeddings for specific layers
- Integration with transformer blocks

